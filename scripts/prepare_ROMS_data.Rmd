---
title: "Organizing ROMS data for SDM fitting"
author: "Owen Liu"
date: "5/11/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidync)
library(here)
library(zoo)
library(lubridate)
library(here)
library(ncdf4)
library(raster)
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose

The purpose of this script is to import and organize ROMS environmental data, and join it to west coast trawl survey data through customizable scripts for choosing species and derived environmental variables of interest (e.g., 1 month-lagged bottom temperature).

As of May 11, 2021, there are 6 ROMS projection files, reporting either bottom temperature or bottom oxygen associated with either the Hadley, GFDL, or IPSL GCM projections.

# Organize Hindcast Data

The hindcast data includes a timeseries of bottom temperature and bottom oxygen matched to trawl survey locations

```{r}
# trawl survey locations and times
trawl <- read_csv(here::here('data','trawl_survey_locations.csv'),col_types = 'cddddd') %>% 
  mutate(date=as_date(date))

# hindcast netCDF
hindcast_file <- tidync(here::here('data','trawl_roms_matchup_timeseries.nc'))

```

```{r}
# matched ROMS hindcast data
hindcast <- hindcast_file %>% hyper_tibble()

glimpse(hindcast)

# and the trawl stations
trawlID <- hindcast_file %>% activate("D0") %>% hyper_tibble()
glimpse(trawlID)

# join trawl info to hindcast data
hindcast <- hindcast %>% left_join(trawlID, by="station")
glimpse(hindcast)
```

# Organize Projected Data

The projected data corresponding to each of the 3 GCMs (Hadley, GFDL, IPSL) contain either bottom temperature or oxygen, for the entire ROMS grid, and for every day from 1980-2100.

I adapted the code below from Steph Brodie's original work. https://github.com/stephbrodie1/WRAP_Location_CaseStudy/blob/master/Create_ROMS_Rasters.R

The first step is to convert ROMS GCM netcdf files into rasters.

```{r}
# projected netCDFs
#Files provided by Mike Jacox. Use beyond the Packard grounfish climate change project requires permission by Mike Jacox (michael.jacox@noaa.gov)
#Note: 1980-2010 are not observed values. 1980-present should not be compared to observations since the interannual variability will not match up (by design). 

#6 variables: lon, lat, year, month, day, covariate. the 6th variable is the covariate
#2 covariates: bottom temp, bottom oxygen
#3 global climate models: Hadley, IPSL, GFDL

n_cov_models <- 6 # 2 covariates * 3 models
cov <- 6 # position of the covariate of interest in the ncdf
gcm_options <- c("gfdl", "had", "ipsl")

#Dimensions:
#1452 years: 1980 - 2100 (121 years * 12 months)
#1452 months: 1 - 12 (121 years * 12 months)
#44195 days: 1980-2100 (121 years * 365.2479 days per year)
#181 lats: 30 - 48 degrees north @ 0.1 degree resolution
#186 lons: 115.5 - 134 degrees west @ 0.1 degree resolution

n_yr_mth_days <- 44195


#-----Load in Dropbox files----
#note: download files locally

files_long <- list.files(here::here('data','Projection NCDFs'),full.names = TRUE)
files_short <- list.files(here::here('data','Projection NCDFs'),full.names = FALSE)

t <- proc.time()
for (f in c(1:n_cov_models)){ 
  print(files_long[f])
  nc <- nc_open(files_long[f])
  
  # print(nc) #run if you want more details on file
  lat <- ncvar_get(nc, 'lat'); lat <- lat[1,]
  lon <- ncvar_get(nc, 'lon'); lon <- lon[,1]
  year <- ncvar_get(nc, 'year') 
  month <- ncvar_get(nc, 'month')
  day <- ncvar_get(nc,'day')
  roms_ymd <- as.POSIXct(paste(year,month,day,sep='-'),tz='UTC')
  name <-  names(nc$var)[cov]
  
  # this step is slow. 169.131 s on Jameal's iMac
  t2 <- proc.time()
  tmp.array <- ncvar_get(nc,name)
  print(paste0("time to open ncdf: ", proc.time()-t2))
  
  #Forcing resolution our ROMS template (a quick fix to resolve grid edge vs. mid point)
  template <- raster(here::here('data','Bathymetry ETOPO1','template.grd')) 

  #Loop through every time step in ncdf and make a raster file.
  
  dir.create(here::here('data','Projection Rasters','Rasters_2d_daily'),showWarnings = FALSE)
  
  for (i in 1:n_yr_mth_days){
    t3 <- proc.time()
    #create nested folders to save files
    #First create GCM folder
    gcm_name <- unlist(strsplit(files_short[f],"_"))
    index <- which(gcm_name %in% gcm_options)
    gcm_folder <- gcm_name[index]
    dir.create(here::here('data','Projection Rasters','Rasters_2d_daily',gcm_folder), showWarnings = FALSE)
    
    var <- unlist(strsplit(files_short[f],"_"))[1:2]
    folder <- here::here('data','Projection Rasters','Rasters_2d_daily',gcm_folder,paste(var[1],var[2],sep="_"))
    dir.create(folder, showWarnings = FALSE)
    
    r <- raster(t(tmp.array[,,i]),
                xmn=min(lon), xmx=max(lon),
                ymn=min(lat), ymx=max(lat), 
                crs=CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
    r <- flip(r,2)
    r <- raster::resample(r, template, method="bilinear")  
    # plot(r)
    
    # #create nested folders to save files
    # #First create GCM folder
    # gcm_options <- c("gfdl", "had", "ipsl")
    # gcm_name <- unlist(strsplit(files_short[f],"_"))
    # index <- which(gcm_name %in% gcm_options)
    # gcm_folder <- gcm_name[index]
    # dir.create(here::here('data','Projection Rasters','Rasters_2d_daily'),showWarnings = FALSE)
    # dir.create(here::here('data','Projection Rasters','Rasters_2d_daily',gcm_folder), showWarnings = FALSE)
    # 
    # var <- unlist(strsplit(files_short[f],"_"))[1:2]
    # folder <- here::here('data','Projection Rasters','Rasters_2d_daily',gcm_folder,paste(var[1],var[2],sep="_"))
    # dir.create(folder, showWarnings = FALSE)
    # 
    #create name of raster file
    #month_num <- format(as.Date(paste("1666",month[i],"06",sep="-"),"%Y-%m-%d"), "%m") # 1666 and 6 are arbitrary choices, goal is just to extract a 2 digit month
    fname <- paste0(var[1],"_",var[2],"_",gcm_folder,"_date_",roms_ymd[i],".grd")
    #save raster
    writeRaster(r, paste0(folder,"/", fname), overwrite=TRUE)
    rm(r)
    print(paste0("finished: ", fname))
  }
  nc_close(nc)
  print(paste0("time to complete raster generation for 1 model-covariate: ", proc.time()-t3))
}
proc.time()-t

# 
# ## bottom temperature
# t_gfdl_file <- tidync(here::here('data','bt_daily_roms_gfdl_1980_2100.nc')) #tidync("/Users/jameal.samhouri/Documents/Future Seas/bt_daily_roms_gfdl_1980_2100.nc")
# t_hadl_file <- tidync(here::here('data','bt_daily_roms_had_1980_2100.nc'))
# t_ipsl_file <- tidync(here::here('data','bt_daily_roms_ipsl_1980_2100.nc'))
# 
# ## bottom oxygen
# o_gfdl_file <- tidync(here::here('data','oxygen_bottom_daily_roms_gfdl_1980_2100.nc'))
# o_hadl_file <- tidync(here::here('data','oxygen_bottom_daily_roms_had_1980_2100.nc'))
# o_ipsl_file <- tidync(here::here('data','oxygen_bottom_daily_roms_ipsl_1980_2100.nc'))

#glimpse(t_gfdl_file)

# # use this to determine varname. hoping we can varname="bt"
# tidync(t_gfdl_file)
# print(t_gfdl_file) #https://ropensci.org/blog/2019/11/05/tidync/ # we wantbt from 1st grid, lon and lat from 2nd grid, year/month/day from 3rd grid
# 
# nc.data <- nc_open(here::here('data','bt_daily_roms_gfdl_1980_2100.nc'), write=FALSE)
# lat <- ncvar_get(nc.data,'lat'); lat <- lat[1,]
# lon <- ncvar_get(nc.data,'lon'); lon <- lon[,1]
# yr <- ncvar_get(nc.data,'year')
# mth <- ncvar_get(nc.data,'month')
# day <- ncvar_get(nc.data,'day')
# tim <- as.POSIXct(paste(yr,mth,day,sep='-'),tz='UTC')
# name <-  names(nc.data$var)[6]
# 
# t <- proc.time()
# tmp.array <- ncvar_get(nc.data,name)
# proc.time()-t
# 
# 
# # extract netcdf files as expanded tables. takes 15-20min on jameal's imac
# t <- proc.time()
# 
# # this approach gave a tibble with columns bt, longitude, latitude, time. lon/lat were index values, time was a year
# t_gfdl <- t_gfdl_file %>% 
#   activate("D0,D1,D2") %>%
#   hyper_tibble()
# 
# proc.time()-t
# 
# glimpse(t_gfdl)
# 
# t_gfdl2 <- t_gfdl_file %>% 
#   #activate("D0,D1") %>%
#   hyper_tibble()
# 
# # # make it smaller
# # t_gfdl_slice <- t_gfdl %>% slice(1:100000)
# # rm(t_gfdl, t_gfdl_file)
# # write_rds(t_gfdl_slice, here::here('data','t_gfdl_slice.rds'))
# 
# t_gfdl_slice <- read_rds(here::here('data','t_gfdl_slice.rds'))
# glimpse(t_gfdl_slice)

####################################
#### MESSAGE FOR JAMEAL AND OWEN ###
####################################

### GO BACK TO LINE 79. We need to return to Heather Welch's code that Jameal modified for the FATE spatial indicators project, and remember how to extract real lat-long coordinates for each cell rather than just dummy identifiers. We also need to figure out how to make hyper_tibble() extract a calendar date column rather than just a year column called time.


```

# Derive Environmental Variables - Hindcast

We want to derive, from the trawl-matched ROMS outputs, some summarized environmental variables. We start by writing a function that can calculate, for each trawl location and time, a mean value for an environmental variable, at that location, over the past x days, where x can be anything. For the edge case at the beginning of the timeseries, to avoid losing data we will use the 'mean of the past x days or since the beginning of the timeseries.

Since time is an integer day, it make it easier for us to calculate lags. For this function, we assume the dataframe has the exact variable names as the `hindcast` dataframe above.

For calculating lagged means, we use the `zoo::rollapply` function.
```{r}
# using `zoo::rollapply`
append_lag <- function(df,variable=temp_roms,lagdays=30){
  df_out <- df %>% 
    group_by(station) %>% # group the observations by trawl survey station
    arrange(time) %>% 
    # calculate the lag
    mutate("mean_{{variable}}_{{lagdays}}":= rollapply({{variable}}, # variable to lag
                                                       FUN=mean, # function to apply
                                                       align='right', # tells the function to lag, not lead
                                                       width=lagdays, # how big of a window
                                                       fill=NA, # value to fill when an observation doesn't exist
                                                       partial=1, # minimum window size for partial computations
                                                       na.rm=T)) %>% 
    ungroup()
  
  df_out
}
```

A quick test
```{r}
testdf <- hindcast %>% slice(1:100000)
test <- append_lag(testdf)

glimpse(test)
```

Apply to temperature and oxygen for the entire dataset

```{r}
t <- proc.time()
hindcast_lagged_temp_oxy <- hindcast %>% 
  append_lag(variable=temp_roms,lagdays=30) %>% 
  append_lag(variable=oxygen_roms,lagdays=30) %>% 
  ungroup()
proc.time()-t

# took about 25m to calculate both lags
glimpse(hindcast_lagged_temp_oxy)
```

How does concurrent temp/oxygen compare to lagged mean temp/oxy?

```{r}
# take a random subsample so it doesn't take forever to plot
samp <- hindcast_lagged_temp_oxy %>% 
  ungroup() %>% 
  slice_sample(n=10000)
samp %>% 
  ggplot(aes(temp_roms,mean_temp_roms_30))+
  geom_point()+geom_smooth(method='lm')+
  labs(x="Concurrent Temperature",y="30 Day Lagged Mean Temperature")
```

# Derive Environmental Variables - Projections

We want to derive, from the ROMS projection outputs, some summarized environmental variables. We start by writing a function that can calculate, for each grid cell in the ROMS domain, a mean/97.5%/2.5% value for an environmental variable, at that location, between the start.month and end.month, where start.month and end.month can be anything (but months in the range May-October make the most sense because that is when the trawl survey occurs). 

Since time is just a year right now, we take the mean for each year. For this function, we assume the dataframe has the exact variable names as the `t_gfdl_slice` dataframe above.

Also I did not yet make this a function!


```{r}

#----Create May-Oct Average Conditions----
#For each year and variable, load in daily data from May-Oct and average. Output to a new folder. 

nyears <- 121

# JS stopped editing here

variables <- c("oxygen_bottom","temp_bottom")
for (variable in variables){
  print(variable)
  
  for (i in 1:nyears){
    files <- list.files(paste0('~/Dropbox/WRAP Location^3/Rasters_2d_monthly/ipsl/',variable), pattern=".grd" , full.names = T)
    month_idx <- rep(1:12,times=121)
    survey_months <- which(month_idx==4,5,6)
    
    start_indx <- survey_months[i]
    
    apr <- raster(files[start_indx])
    may <- raster(files[start_indx+1])
    jun <- raster(files[start_indx+2])
    spring_r <- mean(apr,may,jun)
    years <- seq(1980,2100,1)
    writeRaster(spring_r,paste0('~/Dropbox/WRAP Location^3/Rasters_2d_Spring/ipsl/',variable,'/',variable,'_ipsl_SpringMean_',years[i],'.grd'), overwrite=TRUE)
  }
}

# t_gfdl_slice_summary <- t_gfdl_slice %>%
#   mutate(
#     grid_id = paste0(latitude,"_",longitude)
#   ) %>%
#   group_by(grid_id, time) %>%
#   summarise(
#     bt_mean = mean(bt),
#     bt_upper = quantile(bt, probs=(0.975)),
#     bt_lower = quantile(bt, probs=(0.025))
#   )
# glimpse(t_gfdl_slice_summary)

```

